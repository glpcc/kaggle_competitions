{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.model1 import Net\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training dataset\n",
    "df = pd.read_csv('data/cleaned_train.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide between train and test data\n",
    "train = df[:800]\n",
    "test = df[800:]\n",
    "# Divide between x,y \n",
    "train_x = torch.tensor(train.iloc[:,3:].values).float()\n",
    "train_y = torch.tensor(train.iloc[:,2].values).float()\n",
    "test_x = torch.tensor(test.iloc[:,3:].values).float()\n",
    "test_y = torch.tensor(test.iloc[:,2].values).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "net = Net(inputs=17,outputs=1,dropout_prob=0.05)\n",
    "# Move all to gpu\n",
    "if torch.cuda.is_available():\n",
    "   net = net.to(\"cuda:0\")\n",
    "   train_x = train_x.to(\"cuda:0\")\n",
    "   train_y = train_y.to(\"cuda:0\")\n",
    "   test_x = test_x.to(\"cuda:0\")\n",
    "   test_y = test_y.to(\"cuda:0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the network for training\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "# Initialize loss function\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "# Initialize optimizer\n",
    "optm = torch.optim.Adam(params=net.parameters(),lr=1e-2)\n",
    "scheduler1 = torch.optim.lr_scheduler.LinearLR(optm,1,0.1,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide tensors into batch sizes\n",
    "train_x = train_x.split(batch_size);\n",
    "train_y = train_y.split(batch_size);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, Loss: 0.4952022135257721, Accuracy: 0.7692307829856873\n",
      "EPOCH: 1, Loss: 0.4941532015800476, Accuracy: 0.7692307829856873\n",
      "EPOCH: 2, Loss: 0.49300670623779297, Accuracy: 0.7692307829856873\n",
      "EPOCH: 3, Loss: 0.49201831221580505, Accuracy: 0.7692307829856873\n",
      "EPOCH: 4, Loss: 0.490939736366272, Accuracy: 0.7692307829856873\n",
      "EPOCH: 5, Loss: 0.4900515377521515, Accuracy: 0.7692307829856873\n",
      "EPOCH: 6, Loss: 0.48919105529785156, Accuracy: 0.7692307829856873\n",
      "EPOCH: 7, Loss: 0.4882494807243347, Accuracy: 0.7692307829856873\n",
      "EPOCH: 8, Loss: 0.4874338209629059, Accuracy: 0.7692307829856873\n",
      "EPOCH: 9, Loss: 0.4867309331893921, Accuracy: 0.7692307829856873\n",
      "EPOCH: 10, Loss: 0.48616519570350647, Accuracy: 0.7802197933197021\n",
      "EPOCH: 11, Loss: 0.48568421602249146, Accuracy: 0.7802197933197021\n",
      "EPOCH: 12, Loss: 0.4850236773490906, Accuracy: 0.7802197933197021\n",
      "EPOCH: 13, Loss: 0.4845559298992157, Accuracy: 0.7802197933197021\n",
      "EPOCH: 14, Loss: 0.4839252531528473, Accuracy: 0.7692307829856873\n",
      "EPOCH: 15, Loss: 0.48351365327835083, Accuracy: 0.7692307829856873\n",
      "EPOCH: 16, Loss: 0.4832119047641754, Accuracy: 0.7692307829856873\n",
      "EPOCH: 17, Loss: 0.48295366764068604, Accuracy: 0.7692307829856873\n",
      "EPOCH: 18, Loss: 0.4826743006706238, Accuracy: 0.7692307829856873\n",
      "EPOCH: 19, Loss: 0.48239555954933167, Accuracy: 0.7692307829856873\n",
      "EPOCH: 20, Loss: 0.48223769664764404, Accuracy: 0.7692307829856873\n",
      "EPOCH: 21, Loss: 0.4820689260959625, Accuracy: 0.7692307829856873\n",
      "EPOCH: 22, Loss: 0.48186740279197693, Accuracy: 0.7692307829856873\n",
      "EPOCH: 23, Loss: 0.48177146911621094, Accuracy: 0.7802197933197021\n",
      "EPOCH: 24, Loss: 0.4816824197769165, Accuracy: 0.7802197933197021\n",
      "EPOCH: 25, Loss: 0.4815778434276581, Accuracy: 0.7802197933197021\n",
      "EPOCH: 26, Loss: 0.48158201575279236, Accuracy: 0.7802197933197021\n",
      "EPOCH: 27, Loss: 0.48160067200660706, Accuracy: 0.7802197933197021\n",
      "EPOCH: 28, Loss: 0.48168450593948364, Accuracy: 0.7802197933197021\n",
      "EPOCH: 29, Loss: 0.4817541539669037, Accuracy: 0.7802197933197021\n",
      "EPOCH: 30, Loss: 0.4818231165409088, Accuracy: 0.7802197933197021\n",
      "EPOCH: 31, Loss: 0.4818538725376129, Accuracy: 0.7802197933197021\n",
      "EPOCH: 32, Loss: 0.48192736506462097, Accuracy: 0.7802197933197021\n",
      "EPOCH: 33, Loss: 0.4820517301559448, Accuracy: 0.7802197933197021\n",
      "EPOCH: 34, Loss: 0.48220357298851013, Accuracy: 0.7802197933197021\n",
      "EPOCH: 35, Loss: 0.4824143052101135, Accuracy: 0.7802197933197021\n",
      "EPOCH: 36, Loss: 0.482670396566391, Accuracy: 0.7802197933197021\n",
      "EPOCH: 37, Loss: 0.48296022415161133, Accuracy: 0.7802197933197021\n",
      "EPOCH: 38, Loss: 0.48314157128334045, Accuracy: 0.7802197933197021\n",
      "EPOCH: 39, Loss: 0.48347482085227966, Accuracy: 0.7802197933197021\n",
      "EPOCH: 40, Loss: 0.48375871777534485, Accuracy: 0.7802197933197021\n",
      "EPOCH: 41, Loss: 0.48402103781700134, Accuracy: 0.7802197933197021\n",
      "EPOCH: 42, Loss: 0.48440292477607727, Accuracy: 0.7802197933197021\n",
      "EPOCH: 43, Loss: 0.4846668541431427, Accuracy: 0.7802197933197021\n",
      "EPOCH: 44, Loss: 0.485042005777359, Accuracy: 0.7802197933197021\n",
      "EPOCH: 45, Loss: 0.4854356646537781, Accuracy: 0.7802197933197021\n",
      "EPOCH: 46, Loss: 0.48581546545028687, Accuracy: 0.7802197933197021\n",
      "EPOCH: 47, Loss: 0.48625749349594116, Accuracy: 0.7802197933197021\n",
      "EPOCH: 48, Loss: 0.4867779314517975, Accuracy: 0.7802197933197021\n",
      "EPOCH: 49, Loss: 0.48702505230903625, Accuracy: 0.7802197933197021\n",
      "EPOCH: 50, Loss: 0.4872455894947052, Accuracy: 0.7692307829856873\n",
      "EPOCH: 51, Loss: 0.48762840032577515, Accuracy: 0.7692307829856873\n",
      "EPOCH: 52, Loss: 0.4879841208457947, Accuracy: 0.7692307829856873\n",
      "EPOCH: 53, Loss: 0.4884251356124878, Accuracy: 0.7692307829856873\n",
      "EPOCH: 54, Loss: 0.4888235926628113, Accuracy: 0.7692307829856873\n",
      "EPOCH: 55, Loss: 0.48910635709762573, Accuracy: 0.7692307829856873\n",
      "EPOCH: 56, Loss: 0.48966360092163086, Accuracy: 0.7692307829856873\n",
      "EPOCH: 57, Loss: 0.49027639627456665, Accuracy: 0.7692307829856873\n",
      "EPOCH: 58, Loss: 0.490498811006546, Accuracy: 0.7692307829856873\n",
      "EPOCH: 59, Loss: 0.4910148084163666, Accuracy: 0.7692307829856873\n",
      "EPOCH: 60, Loss: 0.49144476652145386, Accuracy: 0.7692307829856873\n",
      "EPOCH: 61, Loss: 0.4915505051612854, Accuracy: 0.7692307829856873\n",
      "EPOCH: 62, Loss: 0.4920162260532379, Accuracy: 0.7692307829856873\n",
      "EPOCH: 63, Loss: 0.49238696694374084, Accuracy: 0.7692307829856873\n",
      "EPOCH: 64, Loss: 0.4927874207496643, Accuracy: 0.7692307829856873\n",
      "EPOCH: 65, Loss: 0.4932350814342499, Accuracy: 0.7692307829856873\n",
      "EPOCH: 66, Loss: 0.4935277998447418, Accuracy: 0.7692307829856873\n",
      "EPOCH: 67, Loss: 0.49410831928253174, Accuracy: 0.7692307829856873\n",
      "EPOCH: 68, Loss: 0.4945935904979706, Accuracy: 0.7692307829856873\n",
      "EPOCH: 69, Loss: 0.4951018989086151, Accuracy: 0.7692307829856873\n",
      "EPOCH: 70, Loss: 0.4955894351005554, Accuracy: 0.7692307829856873\n",
      "EPOCH: 71, Loss: 0.49600228667259216, Accuracy: 0.7692307829856873\n",
      "EPOCH: 72, Loss: 0.49658119678497314, Accuracy: 0.7692307829856873\n",
      "EPOCH: 73, Loss: 0.497304230928421, Accuracy: 0.7692307829856873\n",
      "EPOCH: 74, Loss: 0.49766844511032104, Accuracy: 0.7692307829856873\n",
      "EPOCH: 75, Loss: 0.4980323910713196, Accuracy: 0.7692307829856873\n",
      "EPOCH: 76, Loss: 0.49828028678894043, Accuracy: 0.7692307829856873\n",
      "EPOCH: 77, Loss: 0.49823760986328125, Accuracy: 0.7692307829856873\n",
      "EPOCH: 78, Loss: 0.4987371265888214, Accuracy: 0.7692307829856873\n",
      "EPOCH: 79, Loss: 0.49907973408699036, Accuracy: 0.7692307829856873\n",
      "EPOCH: 80, Loss: 0.4992470145225525, Accuracy: 0.7692307829856873\n",
      "EPOCH: 81, Loss: 0.4999845027923584, Accuracy: 0.7692307829856873\n",
      "EPOCH: 82, Loss: 0.5001413226127625, Accuracy: 0.7692307829856873\n",
      "EPOCH: 83, Loss: 0.500080406665802, Accuracy: 0.7692307829856873\n",
      "EPOCH: 84, Loss: 0.5004259347915649, Accuracy: 0.7692307829856873\n",
      "EPOCH: 85, Loss: 0.5009289383888245, Accuracy: 0.7692307829856873\n",
      "EPOCH: 86, Loss: 0.50113445520401, Accuracy: 0.7692307829856873\n",
      "EPOCH: 87, Loss: 0.5014356970787048, Accuracy: 0.7692307829856873\n",
      "EPOCH: 88, Loss: 0.5017547011375427, Accuracy: 0.7692307829856873\n",
      "EPOCH: 89, Loss: 0.5019060373306274, Accuracy: 0.7692307829856873\n",
      "EPOCH: 90, Loss: 0.5019504427909851, Accuracy: 0.7692307829856873\n",
      "EPOCH: 91, Loss: 0.5023517608642578, Accuracy: 0.7692307829856873\n",
      "EPOCH: 92, Loss: 0.502636194229126, Accuracy: 0.7692307829856873\n",
      "EPOCH: 93, Loss: 0.5028249621391296, Accuracy: 0.7692307829856873\n",
      "EPOCH: 94, Loss: 0.5030158162117004, Accuracy: 0.7692307829856873\n",
      "EPOCH: 95, Loss: 0.5032437443733215, Accuracy: 0.7692307829856873\n",
      "EPOCH: 96, Loss: 0.5032354593276978, Accuracy: 0.7692307829856873\n",
      "EPOCH: 97, Loss: 0.5035943984985352, Accuracy: 0.7692307829856873\n",
      "EPOCH: 98, Loss: 0.503619372844696, Accuracy: 0.7692307829856873\n",
      "EPOCH: 99, Loss: 0.5038584470748901, Accuracy: 0.7692307829856873\n",
      "EPOCH: 100, Loss: 0.5039987564086914, Accuracy: 0.7692307829856873\n",
      "EPOCH: 101, Loss: 0.5043450593948364, Accuracy: 0.7692307829856873\n",
      "EPOCH: 102, Loss: 0.5042176246643066, Accuracy: 0.7692307829856873\n",
      "EPOCH: 103, Loss: 0.5042485594749451, Accuracy: 0.7692307829856873\n",
      "EPOCH: 104, Loss: 0.5045725107192993, Accuracy: 0.7692307829856873\n",
      "EPOCH: 105, Loss: 0.5046140551567078, Accuracy: 0.7692307829856873\n",
      "EPOCH: 106, Loss: 0.5049414038658142, Accuracy: 0.7582417726516724\n",
      "EPOCH: 107, Loss: 0.5052788257598877, Accuracy: 0.7582417726516724\n",
      "EPOCH: 108, Loss: 0.5052785873413086, Accuracy: 0.7582417726516724\n",
      "EPOCH: 109, Loss: 0.505230188369751, Accuracy: 0.7582417726516724\n",
      "EPOCH: 110, Loss: 0.5059842467308044, Accuracy: 0.7582417726516724\n",
      "EPOCH: 111, Loss: 0.5063942074775696, Accuracy: 0.7582417726516724\n",
      "EPOCH: 112, Loss: 0.5065503716468811, Accuracy: 0.7582417726516724\n",
      "EPOCH: 113, Loss: 0.5067422986030579, Accuracy: 0.7582417726516724\n",
      "EPOCH: 114, Loss: 0.5071253180503845, Accuracy: 0.7582417726516724\n",
      "EPOCH: 115, Loss: 0.5076829195022583, Accuracy: 0.7582417726516724\n",
      "EPOCH: 116, Loss: 0.5073278546333313, Accuracy: 0.7582417726516724\n",
      "EPOCH: 117, Loss: 0.507293701171875, Accuracy: 0.7582417726516724\n",
      "EPOCH: 118, Loss: 0.5074563026428223, Accuracy: 0.7582417726516724\n",
      "EPOCH: 119, Loss: 0.5076273083686829, Accuracy: 0.7582417726516724\n",
      "EPOCH: 120, Loss: 0.5075554847717285, Accuracy: 0.7582417726516724\n",
      "EPOCH: 121, Loss: 0.5074328780174255, Accuracy: 0.7582417726516724\n",
      "EPOCH: 122, Loss: 0.5075187683105469, Accuracy: 0.7582417726516724\n",
      "EPOCH: 123, Loss: 0.5078793168067932, Accuracy: 0.7582417726516724\n",
      "EPOCH: 124, Loss: 0.5075226426124573, Accuracy: 0.7582417726516724\n",
      "EPOCH: 125, Loss: 0.5077939629554749, Accuracy: 0.7582417726516724\n",
      "EPOCH: 126, Loss: 0.5081271529197693, Accuracy: 0.7582417726516724\n",
      "EPOCH: 127, Loss: 0.5085494518280029, Accuracy: 0.7582417726516724\n",
      "EPOCH: 128, Loss: 0.5087640285491943, Accuracy: 0.7582417726516724\n",
      "EPOCH: 129, Loss: 0.5089442729949951, Accuracy: 0.7582417726516724\n",
      "EPOCH: 130, Loss: 0.5092931389808655, Accuracy: 0.7582417726516724\n",
      "EPOCH: 131, Loss: 0.509454071521759, Accuracy: 0.7582417726516724\n",
      "EPOCH: 132, Loss: 0.5094078183174133, Accuracy: 0.7582417726516724\n",
      "EPOCH: 133, Loss: 0.5098293423652649, Accuracy: 0.7582417726516724\n",
      "EPOCH: 134, Loss: 0.5098625421524048, Accuracy: 0.7582417726516724\n",
      "EPOCH: 135, Loss: 0.510575532913208, Accuracy: 0.7582417726516724\n",
      "EPOCH: 136, Loss: 0.510997474193573, Accuracy: 0.7582417726516724\n",
      "EPOCH: 137, Loss: 0.5112472772598267, Accuracy: 0.7582417726516724\n",
      "EPOCH: 138, Loss: 0.5110884308815002, Accuracy: 0.7582417726516724\n",
      "EPOCH: 139, Loss: 0.511364758014679, Accuracy: 0.7582417726516724\n",
      "EPOCH: 140, Loss: 0.5116562247276306, Accuracy: 0.7582417726516724\n",
      "EPOCH: 141, Loss: 0.5117825865745544, Accuracy: 0.7582417726516724\n",
      "EPOCH: 142, Loss: 0.5120208263397217, Accuracy: 0.7582417726516724\n",
      "EPOCH: 143, Loss: 0.5122845768928528, Accuracy: 0.7582417726516724\n",
      "EPOCH: 144, Loss: 0.512633740901947, Accuracy: 0.7582417726516724\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gonza\\Documents\\kaggle_competitions\\titanic\\script.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gonza/Documents/kaggle_competitions/titanic/script.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     optm\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gonza/Documents/kaggle_competitions/titanic/script.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gonza/Documents/kaggle_competitions/titanic/script.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     optm\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gonza/Documents/kaggle_competitions/titanic/script.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m scheduler1\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gonza/Documents/kaggle_competitions/titanic/script.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\gonza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gonza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gonza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gonza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\gonza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Users\\gonza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:262\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    259\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m    261\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m    263\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    265\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "num_batches = 50\n",
    "test_y = test_y.reshape((test_y.shape[0],1))\n",
    "max_accuracy = 0\n",
    "best_model = copy.deepcopy(net.state_dict())\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    for i in range(num_batches):\n",
    "        indx = random.randint(0,len(train_x)-1)\n",
    "        x = train_x[indx]\n",
    "        y = train_y[indx]\n",
    "        net_out = net.forward(x,x.size()[0])\n",
    "        #print(all(i == net_out[0] for i in net_out))\n",
    "        loss = loss_fn.forward(net_out,y.reshape(y.size()[0],1))\n",
    "        optm.zero_grad()\n",
    "        loss.backward()\n",
    "        optm.step()\n",
    "    scheduler1.step()\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        net_out = net.forward(test_x,test_x.size()[0])\n",
    "        loss = loss_fn.forward(net_out,test_y.reshape(test_y.size()[0],1))\n",
    "        predictions = net_out.round()\n",
    "        accuracy = 1- (predictions - test_y).abs().sum()/test_y.size()[0]\n",
    "        if accuracy > max_accuracy:\n",
    "            best_model = copy.deepcopy(net.state_dict())\n",
    "            max_accuracy = accuracy\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracies)\n",
    "        print(f'EPOCH: {epoch}, Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model,'saved_models/best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
